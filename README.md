# sparkify_etl
This project is for building a simple ETL pipeline code which builds a DB in star schema using song files and user activity log files

## Intro story
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.


## Analytical Goal
Build a database for **fast & easy** for querying song play user activies by analytics team.  
**Star schema** is a good way of optimizing data for this purpose.


## Pros and Cons of Star schema
**Pros** of Star schema are
  - Denormalized
  - Simplifies queries
  - Fast aggregations

**Cons** of Star schema are
  - Issues from denormalization
  - Data integrity
  - Decrease query flexibility
  - N-to-N relationship is hard to support


## Datasets
#### 1. Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/).  
Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

    song_data/A/B/C/TRABCEI128F424C983.json
    song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

    {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}


#### 2. Log Dataset
The second dataset consists of log files in JSON format generated by [event simulator](https://github.com/Interana/eventsim).   
The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.

    log_data/2018/11/2018-11-12-events.json
    log_data/2018/11/2018-11-13-events.json

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.  
![Sample log dataset!](https://s3.amazonaws.com/video.udacity-data.com/topher/2019/February/5c6c15e9_log-data/log-data.png "2018-11-12-events.json")



## Detail schema of song play activies
#### Fact table :
##### 1. **songplays** - records in log data associated with song plays i.e. records with page NextSong
  - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimention tables :
##### 2. **users** - users in the app  
  - user_id, first_name, last_name, gender, level  

##### 3. **songs** - songs in music database  
  - song_id, title, artist_id, year, duration  

##### 4. **artists** - artists in music database  
  - artist_id, name, location, latitude, longitude  

##### 5. **time** - timestamps of records in songplays broken down into specific units  
  - start_time, hour, day, week, month, year, weekday  


## Database build steps
#### 1. Create database object scripts
  - sql_queries.py : Has create database/tables SQL statements, insert statements, select statement, and drop statements
  - create_tables.py : Has codes for creating and dropping tables
  - test.ipynb : Has test codes for making sure if database and tables are created/deleted

#### 2. Build ETL Process
  - etl.ipynb : Develop ETL processes for each table. At the end of each table insert, or at the end of the notebook, run test.ipynb to confirm that records were successfully inserted into each table.

#### 3. ETL pipeline
  - etl.py : Write script for processing whole dataset files using the code in etl.ipynb

## Running ETL pipeline
#### 1. Make sure database and tables created. For creation, run following command in terminal
  - Run **python create_tables.py**

#### 2. For running ETL pipeline, run following command in terminal
  - Run **python etl.py** 
